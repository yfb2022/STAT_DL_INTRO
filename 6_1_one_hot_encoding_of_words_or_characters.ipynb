{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yfb2022/STAT_DL_INTRO/blob/main/6_1_one_hot_encoding_of_words_or_characters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dbLusdaii4h9",
        "outputId": "053445a0-ac2d-415a-a70a-02da81eae2a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFrlc6udi4iC"
      },
      "source": [
        "# One-hot encoding of words or characters\n",
        "\n",
        "원-핫 인코딩은 토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법.\n",
        "\n",
        "이는 모든 단어에 고유한 정수 인덱스를 연결한 다음 이 정수 인덱스 i를 어휘 크기인 N 크기의 이진 벡터로 변환하는 것으로 구성됩니다. 이는 i 번째 항목인 1을 제외하고 모두 0이 됩니다. .\n",
        "\n",
        "물론 문자 수준에서도 원-핫 인코딩을 수행할 수 있습니다.\n",
        "\n",
        "하나는 단어용이고 다른 하나는 문자용입니다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh2DOkO-i4iG"
      },
      "source": [
        "Word level one-hot encoding (toy example):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-ImYxOWji4iH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# This is our initial data; one entry per \"sample\"\n",
        "# (in this toy example, a \"sample\" is just a sentence, but\n",
        "# it could be an entire document).\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# First, build an index of all tokens in the data.\n",
        "token_index = {}\n",
        "# token_index <- list()\n",
        "for sample in samples:\n",
        "    # We simply tokenize the samples via the `split` method.\n",
        "    # in real life, we would also strip punctuation and special characters\n",
        "    # from the samples.\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            # Assign a unique index to each unique word\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # Note that we don't attribute index 0 to anything.\n",
        "\n",
        "# for (sample in samples)\n",
        "#    for (word in strsplit(sample, \" \")[[1]])\n",
        "#        if (!word %in% names(token_index))\n",
        "#           token_index[[word]] =  length(token_index) + 2\n",
        "\n",
        "# Next, we vectorize our samples.\n",
        "# We will only consider the first `max_length` words in each sample.\n",
        "max_length = 10\n",
        "\n",
        "# This is where we store our results:\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "# results <- array(0, dim = c(length(samples), max_length, max(as.integer(token_index))))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1.\n",
        "\n",
        "# for (i in 1:length(samples)) {\n",
        "#     sample <- samples[[i]]\n",
        "#     words <- head(strsplit(sample, \" \")[[1]], n = max_length)\n",
        "#     for (j in 1:length(words)) {\n",
        "#         index <- token_index[[words[[j]]]]\n",
        "#         results[[i, j, index]] <- 1\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcMz4RTi4iK"
      },
      "source": [
        "Character level one-hot encoding (toy example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8VrUxwgti4iL"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable  # All printable ASCII characters.\n",
        "# ascii_tokens <- c(\"\", sapply(as.raw(c(32:126)), rawToChar))\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "# token_index <- c(1: (length(ascii_tokens)))\n",
        "# names(token_index) <- ascii_tokens\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample[:max_length]):\n",
        "        index = token_index.get(character)\n",
        "        results[i, j, index] = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CFdwgB8i4iM"
      },
      "source": [
        "원핫 인코딩 Keras 라이브러리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mDGnhii4iN"
      },
      "source": [
        "Using Keras for word-level one-hot encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUJUP7qWi4iO",
        "outputId": "c1c30ae8-179c-43a2-a96c-84c012feba19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We create a tokenizer, configured to only take\n",
        "# into account the top-1000 most common words\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "# You could also directly get the one-hot binary representations.\n",
        "# Note that other vectorization modes than one-hot encoding are supported!\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "# This is how you can recover the word index that was computed\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ0Pix4si4iP"
      },
      "source": [
        "\n",
        "원-핫 인코딩의 변형은 소위 \"원-핫 해싱 트릭\"입니다. 이는 어휘의 고유 토큰 수가 너무 커서 명시적으로 처리할 수 없을 때 사용할 수 있습니다.\n",
        "\n",
        "각 단어에 인덱스를 명시적으로 할당하고 사전에 이러한 인덱스에 대한 참조를 유지하는 대신 단어를 고정된 크기의 벡터로 해시할 수 있습니다.이는 일반적으로 매우 가벼운 해싱 함수를 사용하여 수행됩니다.\n",
        "\n",
        "이 방법의 가장 큰 장점은 명시적인 단어 인덱스를 유지하지 않아도 되므로 메모리가 절약되고 데이터의 온라인 인코딩이 가능하다는 것입니다(사용 가능한 모든 데이터를 보기 전에 즉시 토큰 벡터 생성 시작). 이 방법의 한 가지 단점은 \"해시 충돌\"에 취약하다는 것입니다. 두 개의 서로 다른 단어가 동일한 해시로 끝날 수 있으며 결과적으로 이러한 해시를 보는 모든 기계 학습 모델은 이러한 단어 간의 차이를 구분할 수 없습니다. 해시 공간의 차원이 해시되는 고유 토큰의 총 수보다 훨씬 클 경우 해시 충돌 가능성이 감소합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0S3LkM-i4iP"
      },
      "source": [
        "Word-level one-hot encoding with hashing trick (toy example):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iH0wOTT-i4iQ"
      },
      "outputs": [],
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We will store our words as vectors of size 1000.\n",
        "# Note that if you have close to 1000 words (or more)\n",
        "# you will start seeing many hash collisions, which\n",
        "# will decrease the accuracy of this encoding method.\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # Hash the word into a \"random\" integer index\n",
        "        # that is between 0 and 1000\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        # index <- abs(spooky.32(words[[i]])) %% dimensionality\n",
        "        results[i, j, index] = 1."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}